
% Copyright 2020 by Robert Hildebrand
%This work is licensed under a
%Creative Commons Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)
%See http://creativecommons.org/licenses/by-sa/4.0/

%\documentclass[../open-optimization/open-optimization.tex]{subfiles}
%
%
%\begin{document}

\chapter{NLP Algorithms}
\todoChapter{ 50\% complete. Goal 80\% completion date: November 20\\
Notes: }


\section{Algorithms Introduction}

We will begin with unconstrained optimization and consider several different algorithms based on what is known about the objective function.  In particular, we will consider the cases where we use
\begin{itemize}
\item Only function evaluations (also known as \emph{derivative free optimization}),
\item Function and gradient evaluations,
\item Function, gradient, and hessian evaluations.
\end{itemize}

We will first look at these algorithms and their convergence rates in the 1-dimensional setting and then extend these results to higher dimensions.

\section{1-Dimensional Algorithms}
We suppose that we solve the problem 

\begin{align}
\min f(x)\\
x \in [a,b].
\end{align}

That is, we minimize the univariate function $f(x)$ on the interval $[l,u]$.  

For example, 

\begin{align}
\min (x^2 - 2)^2\\
 0 \leq x \leq 10.
\end{align}

Note, the optimal solution lies at $x^* = \sqrt{2}$, which is an irrational number.  Since we will consider algorithms using floating point precision, we will look to return a solution $\bar x$ such that $\|x^* - \bar x\| < \epsilon$ for some small $\epsilon > 0$, for instance, $\epsilon = 10^{-6}$.


\section*{Golden Section Search} % --------------------------------------------

A function $f:[a,b]\rightarrow\mathbb{R}$ satisfies the \emph{unimodal property} if it has exactly one local minimum and is monotonic on either side of the minimizer.
In other words, $f$ decreases from $a$ to its minimizer $x^*$, then increases up to $b$ (see Figure \ref{fig:1dopt-golden-unimodal}).
% Unimodal functions are the ``easiest'' kinds of functions to optimize, but there are several strategies for doing so.
The \emph{golden section search} method optimizes a unimodal function $f$ by iteratively defining smaller and smaller intervals containing the unique minimizer $x^*$.
This approach is especially useful if the function's derivative does not exist, is unknown, or is very costly to compute.

By definition, the minimizer $x^*$ of $f$ must lie in the interval $[a,b]$.
To shrink the interval around $x^*$, we test the following strategically chosen points.
\begin{equation*}
\tilde{a} = b - \frac{b - a}{\varphi}\qquad\qquad
\tilde{b} = a + \frac{b - a}{\varphi}
% \label{eq:1dopt-golden-testpts}
\end{equation*}
Here $\varphi = \frac{1 + \sqrt{5}}{2}$ is the \emph{golden ratio}.
At each step of the search, $[a,b]$ is refined to either $[a,\tilde{b}]$ or $[\tilde{a}, b]$, called the \emph{golden sections}, depending on the following criteria.
 % \footnote{Note that this is the same as $\tilde{a} = a + \rho(b-a)$ and $\tilde{b} = a + (1-\rho)(b-a)$ where $\rho = \frac{3-\sqrt{5}}{2}$.}
% These points define evenly sized intervals $[\tilde{a}, b]$ and $[a, \tilde{b}]$, called \emph{golden sections} since the ratio of the lengths of $[a,\tilde{a}]$ to $[\tilde{a},b]$ is the same as the ratio of the lengths of $[\tilde{a}, b]$ and $[a, b]$.
% Additionally, the property of the golden ratio that  $\frac{1}{\varphi^2} = 1 - \frac{1}{\varphi}$ leads to a smaller amount of computations needed.

If $f(\tilde{a}) < f(\tilde{b})$, then since $f$ is unimodal, it must be increasing in a neighborhood of $\tilde{b}$.
The unimodal property also guarantees that $f$ must be increasing on $[\tilde{b}, b]$ as well, so $x^* \in [a, \tilde{b}]$ and we set $b = \tilde{b}$.
By similar reasoning, if $f(\tilde{a}) > f(\tilde{b})$, then $x^* \in [\tilde{a}, b]$ and we set $a = \tilde{a}$.
If, however, $f(\tilde{a}) = f(\tilde{b})$, then the unimodality of $f$ does not guarantee anything about where the minimizer lies.
Assuming either $x^*\in [a, \tilde{b}]$ or $x^* \in [\tilde{a}, b]$ allows the iteration to continue, but the method is no longer guaranteed to converge to the local minimum.

At each iteration, the length of the search interval is divided by $\varphi$.
The method therefore converges linearly, which is somewhat slow.
However, the idea is simple and each step is computationally inexpensive.

\begin{figure}[H] % Golden section search for unimodal optimization.
    \centering
    \includegraphics[width=.7\textwidth]{golden_section.pdf}
    \caption{The unimodal $f:[a,b]\rightarrow\mathbb{R}$ can be minimized with a golden section search.
    For the first iteration, $f(\tilde{a}) < f(\tilde{b})$, so $x^* \in [a, \tilde{b}]$.
    New values of $\tilde{a}$ and $\tilde{b}$ are then calculated from this new, smaller interval.}
    \label{fig:1dopt-golden-unimodal}
\end{figure}

\begin{algorithm}[H]
\begin{algorithmic}[1]
\Procedure{golden\_section}{$f$,\ $a$,\ $b$,\ \li{tol},\ \li{maxiter}}
   \State $x_0 \gets (a + b)/2$
        \Comment{Set the initial minimizer approximation as the interval midpoint.}
   \State $\varphi = (1+\sqrt{5})/2$
        % \Comment{Set $\varphi$.}
   \For{$i=1,2,\dots,$\ \li{maxiter}}
        \Comment Iterate only \li{maxiter} times at most.
        \State $c \gets (b - a)/\varphi$
        \State $\tilde{a} \gets b - c$
            % \Comment{Set $\tilde{a}$.}
        \State $\tilde{b} \gets a + c$
            % \Comment{Set $\tilde{b}$. }
        \If{$f(\tilde{a}) \leq f(\tilde{b})$}
            \Comment{Get new boundaries for the search interval.}
            \State $b \gets \tilde{b}$
         \Else{}
            \State $a \gets \tilde{a}$
                \EndIf
        \State $x_1 \gets (a + b)/2$
            \Comment{Set the minimizer approximation as the interval midpoint.}
        \If{$|x_0-x_1| <$ \li{tol}}
              \State \texttt{break}
                \Comment{Stop iterating if the approximation stops changing enough.}
       \EndIf
        \State $x_0 \gets x_1$
   \EndFor
   \State \pseudoli{return} $x_1$
\EndProcedure
\end{algorithmic}
\caption{The Golden Section Search}
\label{Alg:Golden-Section-Search}
\end{algorithm}

\begin{problem}{Implement golden search.}
Write a function that accepts a function $f:\mathbb{R}\rightarrow\mathbb{R}$, interval limits $a$ and $b$, a stopping tolerance \li{tol}, and a maximum number of iterations \li{maxiter}.
Use Algorithm \ref{Alg:Golden-Section-Search} to implement the golden section search.
Return the approximate minimizer $x^*$, whether or not the algorithm converged (\li{true} or \li{false}), and the number of iterations computed.

Test your function by minimizing $f(x) = e^x - 4x$ on the interval $[0, 3]$, then plotting the function and the computed minimizer together.
Also compare your results to SciPy's golden section search, \li{scipy.optimize.golden()}.

\begin{lstlisting}
>>> from scipy import optimize as opt
>>> import numpy as np

>>> f = lambda x : np.exp(x) - 4*x
>>> opt.golden(f, brack=(0,3), tol=.001)
1.3862578679031485              # ln(4) is the minimizer.
\end{lstlisting}
\label{prob:golden-section-search}
\end{problem}




\subsection{Golden Search Method - Derivative Free Algorithm}
\begin{resource}
\begin{itemize}
\item \href{https://www.youtube.com/watch?v=hLm8xfwWYPw}{Youtube! - Golden Section Search Method}
\end{itemize}
\end{resource}

Suppose that $f(x)$ is unimodal on the interval $[a,b]$, that is, it is a continuous function that has a single minimizer on the interval.  

Without any extra information, our best guess for the optimizer is $\bar x = \tfrac{a+ b}{2}$ with a maximum error of $\epsilon = \tfrac{b-a}{2}$.    Our goal is to reduce the size of the interval where we know $x^*$ to be, and hence improve our best guess and the maximum error of our guess.


 Now we want to choose points in the interior of the interval to help us decide where the minimizer is.  Let $x_1, x_2$ such that 

$$
a < x_2 < x_1 < b.
$$

Next, we evaluate the function at these four points.  Using this information, we would like to argue a smaller interval in which $x^*$ is contained.  In particular, since $f$ is unimodal, it must hold that 
\begin{enumerate}
\item $x^* \in [a, x_2]$ if  $f(x_1) \leq f(x_2)$, 
\item $x^* \in [x_1,b]$ if $f(x_2) < f(x_1)$,
\end{enumerate}


After comparing these function values, we can reduce the size of the interval and hence reduce the region where we think $x^*$ is.   


We will now discuss how to chose $x_1,x_2$ in a way that we can 
\begin{enumerate}
\item Reuse function evaluations,
\item Have a constant multiplicative reduction in the size of the interval.
\end{enumerate}


We consider the picture:\\
%\begin{center}
%\includegraphics[scale = 0.4]{golden-search-explain}\\
%\end{center}
%In case 1, we remove the right side:\\
%\begin{center}
%\includegraphics[scale = 0.4]{golden-search-explain1}\\
%\end{center}
%In case 2, we remove the left side:\\
%\begin{center}
%\includegraphics[scale = 0.4]{golden-search-explain2}\\
%\end{center}

To determine the best $d$, we want to decrease by a constant factor.  Hence, we decrease be a factor $\gamma$, which we will see is the golden ration (GR).  To see this, we assume that $(b-a) = 1$, and ask that $d = \gamma$.  Thus, $x_1 - a = \gamma$ and $b - x_2 = \gamma$.  If we are in case 1, then we cut off $b-x_1 = 1-\gamma$.  Now, if we iterate and do this again, we will have an initial length of $\gamma$ and we want to cut off the interval $x_2 - x_1$ with this being a proportion of $(1-\gamma)$ of the remaining length.  Hence, the second time we will cut of $(1-\gamma)\gamma$, which we set as the length between $x_1$ and $x_2$.  

Considering the geometry, we have
$$
\text{ length $a$ to $x_1$ } + \text{length $x_2$ to $b$} = \text{ total length } + \text{ length $x_2$ to $x_2$}
$$ 
hence
$$
\gamma + \gamma = 1 + (1-\gamma)\gamma.
$$
Simplifying, we have
$$
\gamma^2 + \gamma - 1 = 0.
$$
Applying the quadratic formula, we see
$$
\gamma = \frac{-1 \pm \sqrt{5}}{2}.
$$
Since we want $\gamma > 0$, we take 
$$
\gamma = \frac{-1 + \sqrt{5}}{2} \approx 0.618
$$
This is exactly the Golden Ratio (or, depending on the definition, the golden ration minus 1).



\subsubsection{Example:}

%\begin{center}
%\includegraphics[scale = 0.4]{golden-search-example1}\\
%Initial setup:\\
%\includegraphics[scale = 0.4]{golden-search-example2}\\
%Step 1:\\
%\includegraphics[scale = 0.4]{golden-search-example3}\\
%Step 2:\\
%\includegraphics[scale = 0.4]{golden-search-example4}\\
%Step 3:\\
%\includegraphics[scale = 0.4]{golden-search-example5}\\
%Step 4:\\
%\includegraphics[scale = 0.4]{golden-search-example6}
%\end{center}
We can conclude that the optimal solution is in $[1.4,3.8]$, so we would guess the midpoint $\bar x = 2.6$ as our approximate solution with a maximum error of $\epsilon = 1.2$.

\begin{general}{Convergence Analysis of Golden Search Method}{}
After $t$ steps of the Golden Search Method, the interval in question will be of length
$$
(b-a)(GR)^t \approx (b-a)(0.618)^t
$$
Hence, by guessing the midpoint, our worst error could be
$$
\frac{1}{2}(b-a)(0.618)^t.
$$
\end{general}






\subsection{Bisection Method - 1st Order Method (using Derivative)}

\subsubsection{Minimization Interpretation}
\textbf{Assumptions: $f$ is convex, differentiable}\\

We can look for a minimizer of the function $f(x)$ on the interval $[a,b]$.


\subsubsection{Root finding Interpretation}
Instead of minimizing, we can look for a root of $f'(x)$.  That is, find $x$ such that $f'(x) = 0$.  

\textbf{Assumptions: $f'(a)<0 <  f'(b)$, OR, $f'(b) < 0 < f'(a)$.  $f'$ is continuous}\\
The goal is to find a root of the function $f'(x)$ on the interval $[a,b]$.  If $f$ is convex, then we know that this root is indeed a global minimizer.


Note that if $f$ is convex, it only makes sense to have the assumption $f'(a)<0 <  f'(b)$.






\begin{general}{Convergence Analysis of Bisection Method}{}
After $t$ steps of the Bisection Method, the interval in question will be of length
$$
(b-a)\left(\frac{1}{2}\right)^t.
$$
Hence, by guessing the midpoint, our worst error could be
$$
\frac{1}{2}(b-a)\left(\frac{1}{2}\right)^t.
$$
\end{general}

\subsection{Gradient Descent - 1st Order Method (using Derivative)}
\textbf{Input:} $f(x)$, $\nabla f(x)$, initial guess $x^0$, learning rate $\alpha$, tolerance $\epsilon$\\
\textbf{Output:} An approximate solution $x$

\begin{enumerate}
\item Set $t= 0$
\item While $\| f(x^t)\|_2 > \epsilon$:
\begin{enumerate}
\item Set $x^{t+1} \leftarrow x^t- \alpha \nabla f(x^t)$.
\item Set $t \leftarrow t+1$.
\end{enumerate}
\item Return $x^{t}$.
\end{enumerate}


\section*{Newton's Method} % ==================================================

\emph{Newton's method} is an important root-finding algorithm that can also be used for optimization.
Given $f:\mathbb{R}\rightarrow\mathbb{R}$ and a good initial guess $x_0$, the sequence $(x_k)_{k=1}^\infty$ generated by the recursive rule
\[
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
\]
converges to a point $\bar{x}$ satisfying $f(\bar{x}) = 0$.
The first-order necessary conditions from elementary calculus state that if $f$ is differentiable, then its derivative evaluates to zero at each of its local minima and maxima.
Therefore using Newton's method to find the zeros of $f'$ is a way to identify potential minima or maxima of $f$.
Specifically, starting with an initial guess $x_0$, set
\begin{equation}
x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}
\label{eq:1dopt-newton}
\end{equation}
and iterate until $|x_k - x_{k-1}|$ is satisfactorily small.
Note that this procedure does not use the actual function $f$ at all, but it requires many evaluations of its first and second derivatives.
As a result, Newton's method converges in few iterations, but it can be computationally expensive.

Each step of \eqref{eq:1dopt-newton} can be thought of approximating the objective function $f$ by a quadratic function $q$ and finding its unique extrema.
That is, we first approximate $f$ with its second-degree Taylor polynomial centered at $x_k$.
\[
q(x) = f(x_k) + f'(x_k) (x - x_k) + \frac{1}{2} f''(x_k) (x - x_k)^2
\]
This quadratic function satisfies $q(x_k) = f(x_k)$ and matches $f$ fairly well close to $x_k$.
Thus the optimizer of $q$ is a reasonable guess for an optimizer of $f$.
To compute that optimizer, solve $q'(x) = 0$.
\[
0 = q'(x) = f'(x_k) + f''(x_k)(x - x_k)
\qquad\Longrightarrow\qquad
x = x_k - \frac{f'(x_k)}{f''(x_k)}
\]
This agrees with \eqref{eq:1dopt-newton} using $x_{k+1}$ for $x$.
See Figure \ref{fig:1dopt-quadratic-newton}.

\begin{figure}[H]
\centering
\includegraphics[width = .7 \textwidth]{quad_approx.pdf}
\caption{A quadratic approximation of $f$ at $x_k$.
The minimizer $x_{k+1}$ of $q$ is close to the minimizer of $f$.
}
\label{fig:1dopt-quadratic-newton}
\end{figure}

Newton's method for optimization works well to locate minima when $f''(x) > 0$ on the entire domain.
However, it may fail to converge to a minimizer if $f''(x) \le 0$ for some portion of the domain.
If $f$ is not unimodal, the initial guess $x_0$ must be sufficiently close to a local minimizer $x^*$ in order to converge.

\begin{problem}{Newton's method for optimization}{}
Let $f:\mathbb{R}\rightarrow\mathbb{R}$.
Write a function that accepts $f'$, $f''$, a starting point $x_0$, a stopping tolerance \li{tol}, and a maximum number of iterations \li{maxiter}.
Implement Newton's method using \eqref{eq:1dopt-newton} to locate a local optimizer.
Return the approximate optimizer, whether or not the algorithm converged, and the number of iterations computed.

Test your function by minimizing $f(x) = x^2 + \sin(5x)$ with an initial guess of $x_0 = 0$.
Compare your results to \li{scipy.optimize.newton()}, which implements the root-finding version of Newton's method.
\begin{lstlisting}
>>> df = lambda x : 2*x + 5*np.cos(5*x)
>>> d2f = lambda x : 2 - 25*np.sin(5*x)
>>> opt.newton(df, x0=0, fprime=d2f, tol=1e-10, maxiter=500)
-1.4473142236328096
\end{lstlisting}
Note that other initial guesses can yield different minima for this function.
\end{problem}


\paragraph{Convervence of Newton's Method}

We consider the function $f(x) = e^x + e^{-x}$.

\begin{tabular}{lrrrrr}
\toprule
{} &           $x$ &      $f(x)$ &  $f^\prime(x)$ &  $|x^k - x^{k-1}|$ &  $\frac{|x^ k - x^{k-1}|}{|x^{k-1} - x^{k-2}|}$ \\
\midrule
0 &  6.100000e+00 &  445.860013 &   4.458555e+02 &                NaN &                                            NaN \\
0 &  5.100010e+00 &  164.029654 &   1.640175e+02 &       9.999899e-01 &                                            NaN \\
0 &  4.100084e+00 &   60.361952 &   6.032881e+01 &       9.999257e-01 &                                   9.999357e-01 \\
0 &  3.100633e+00 &   22.257038 &   2.216700e+01 &       9.994509e-01 &                                   9.995252e-01 \\
0 &  2.104679e+00 &    8.326354 &   8.082584e+00 &       9.959545e-01 &                                   9.965016e-01 \\
0 &  1.133956e+00 &    3.429685 &   2.786169e+00 &       9.707231e-01 &                                   9.746662e-01 \\
0 &  3.215870e-01 &    2.104313 &   6.543175e-01 &       8.123688e-01 &                                   8.368697e-01 \\
0 &  1.064582e-02 &    2.000113 &   2.129203e-02 &       3.109412e-01 &                                   3.827587e-01 \\
0 &  4.021572e-07 &    2.000000 &   8.043143e-07 &       1.064541e-02 &                                   3.423609e-02 \\
0 & -6.935643e-17 &    2.000000 &  -1.110223e-16 &       4.021572e-07 &                                   3.777751e-05 \\
0 & -1.384528e-17 &    2.000000 &   0.000000e+00 &       5.551115e-17 &                                   1.380335e-10 \\
0 & -1.384528e-17 &    2.000000 &   0.000000e+00 &       0.000000e+00 &                                   0.000000e+00\\
\hline
\end{tabular}


\subsection{Newton's Method - 2nd Order Method (using Derivative and Hessian)}

\textbf{Input:} $f(x)$, $\nabla f(x)$, $\nabla^2f(x)$, initial guess $x^0$, learning rate $\alpha$, tolerance $\epsilon$\\
\textbf{Output:} An approximate solution $x$

\begin{enumerate}
\item Set $t= 0$
\item While $\| f(x^t)\|_2 > \epsilon$:
\begin{enumerate}
\item Set $x^{t+1} \leftarrow x^t - \alpha [\nabla^2 f(x^t)]^{-1} \nabla f(x^t)$.
\item Set $t \leftarrow t+1$.
\end{enumerate}
\item Return $x^t$.
\end{enumerate}


\begin{theorem}{Rate of Convergence of Second-Order Methods in Continuous Optimization}{}
Let $ f: \mathbb{R}^n \rightarrow \mathbb{R} $ be a twice continuously differentiable function and $ x^* $ be a local minimum of $ f $. If the Hessian matrix $ \nabla^2 f(x^*) $ is positive definite, and if the iteration scheme is based on a quadratic model of the objective function (like Newton's method), then the method is locally quadratically convergent. Specifically, there exists a neighborhood $ N $ of $ x^* $ such that for all starting points $ x_0 $ in $ N $, the error at iteration $ k+1 $ satisfies:
\[
\| x_{k+1} - x^* \| \leq C \| x_k - x^* \|^2
\]
for some constant $ C > 0 $.
\end{theorem}

\begin{proof}
Assuming that we're working with Newton's method, the iteration is given by:
\[
x_{k+1} = x_k - \left( \nabla^2 f(x_k) \right)^{-1} \nabla f(x_k)
\]

1. Taylor's expansion of $ f $ about $ x^* $ gives:
\[
f(x) \approx f(x^*) + \nabla f(x^*)^T (x - x^*) + \frac{1}{2} (x - x^*)^T \nabla^2 f(x^*) (x - x^*)
\]

2. Since $ x^* $ is a local minimum and the gradient vanishes at $ x^* $, the first-order term disappears, and we have:
\[
f(x) \approx f(x^*) + \frac{1}{2} (x - x^*)^T \nabla^2 f(x^*) (x - x^*)
\]

3. Using Newton's iteration, the update becomes:
\[
x_{k+1} - x^* = (x_k - x^*) - \left( \nabla^2 f(x_k) \right)^{-1} \nabla f(x_k)
\]

4. As $ f $ is twice continuously differentiable and $ \nabla^2 f(x^*) $ is positive definite, there exists a neighborhood $ N $ of $ x^* $ where the Hessian is invertible and Lipschitz continuous. Thus, for $ x_k $ close enough to $ x^* $, the Hessian at $ x_k $ will be close to the Hessian at $ x^* $.

5. Consequently, for $ x_k $ close enough to $ x^* $, $ \nabla^2 f(x_k) $ will also be positive definite, ensuring that Newton's direction is a descent direction.

6. From the Taylor expansion, the gradient $ \nabla f(x_k) $ is approximately proportional to $ x_k - x^* $, and the Hessian is roughly constant. Hence, the update step in the Newton iteration will be proportional to the square of the error:
\[
x_{k+1} - x^* \propto (x_k - x^*)^2
\]

From the above, we see that the error at iteration $ k+1 $ is bounded by the square of the error at iteration $ k $, proving the local quadratic convergence.
\end{proof}


\section{Multi-Variate Unconstrained Optimizaition}
We will now use the techniques for 1-Dimensional optimization and extend them to multi-variate case.  We will begin with unconstrained versions (or at least, constrained to a large box) and then show how we can apply these techniques to constrained optimization.

\subsection{Descent Methods - Unconstrained Optimization - Gradient, Newton} 

\begin{general}{Outline for Descent Method for Unconstrained Optimization}{}
\textbf{Input:} 
\begin{itemize}
\item A function $f(x)$
\item Initial solution $x^0$
\item Method for computing step direction $d_t$
\item Method for computing length $t$ of step
\item Number of iterations $T$
\end{itemize}

\textbf{Output:}
\begin{itemize}
\item A point $x_{T}$ (hopefully an approximate minimizer)
\end{itemize}

\textbf{Algorithm}
\begin{enumerate}
\item For $t=1, \dots, T$,
$$
\text{ set } x_{t+1} = x_t + \alpha_t d_t 
$$
\end{enumerate}

\end{general}
\subsubsection{Choice of $\alpha_t$}
There are many different ways to choose the step length $\alpha_t$.  Some choices have proofs that the algorithm will converge quickly.  An easy choice is to have a constant step length $\alpha_t = \alpha$, but this may depend on the specific problem.

\subsubsection{Choice of $d_t$ using $\nabla f(x)$}
Choice of descent methods using $\nabla f(x)$ are known as \emph{first order methods}.
Here are some choices:
\begin{enumerate}
\item \textbf{Gradient Descent:}   $d_t = - \nabla f(x_t)$
\item \textbf{Nesterov Accelerated Descent:} $d_t = \mu (x_t - x_{t-1}) - \gamma \nabla f(x_t + \mu(x_t- x_{t-1}))$
\end{enumerate}
Here, $\mu, \gamma$ are some numbers.  The number $\mu$ is called the momentum.  

\subsection{Stochastic Gradient Descent - The mother of all algorithms.}
 A popular method is called \emph{stochastic gradient descent} (SGD).   This has been described as "The mother of all algorithms".  
 This is a method to \textbf{approximate the gradient} typically used in machine learning or stochastic programming settings.  
 
\begin{general}{Stochastic Gradient Descent}{}

Suppose we want to solve
\begin{equation}
\min_{x \in \R^n} F(x) = \sum_{i=1}^N f_i(x).
\end{equation}

We could use \emph{gradient descent} and have to compute the gradient $\nabla F(x)$ at each iteration.  But!   We see that in the \textbf{cost to compute the gradient} is roughly $O(nN)$, that is, it is very dependent on the number of function $N$, and hence each iteration will take time dependent on $N$.

\textbf{Instead!} Let $i$ be a uniformly random sample from $\{1, \dots, N\}$.  Then we will use $\nabla f_i(x)$ as an approximation of $\nabla F(x)$.  Although we lose a bit by using a guess of the gradient, this approximation only takes $O(n)$ time to compute.  And in fact, in expectation, we are doing the same thing.  That is,

$$
N\cdot  \mathbb{E}( \nabla f_i(x)) = N \sum_{i=1}^N \frac{1}{N} \nabla f_i(x) =  \sum_{i=1}^N\nabla f_i(x) = \nabla \left(  \sum_{i=1}^N   f_i(x)\right) = \nabla F(x).
$$


Hence, the SGD algorithm is:

\begin{enumerate}
\item Set $t = 0$
\item While ...(some stopping criterion)
\begin{enumerate}
\item Choose $i$ uniformly at random in $\{1, \dots, N\}$.
\item Set $d_t = \nabla f_i(x_t)$
\item Set $x_{t+1} = x_t - \alpha d_t$
\end{enumerate}
\end{enumerate}

There can be many variations on how to decide which functions $f_i$ to evaluate gradient information on.  Above is just one example.

\end{general}
 
 
 
 Linear regression is an excellent example of this.  

\begin{example}{Linear Regression with SGD}{}
Given data points $x^1, \dots, x^N \in \R^d$ and output $y^1, \dots, y^N \in \R$, find $a \in \R^d$ and $b \in \R$ such that $a^\top x^i + b \approx y^i$.   This can be written as the optimization problem 
\begin{equation}
\begin{array}{rl}
\min_{a,b} \quad & \sum_{i=1}^N g_i(a,b)
\end{array}
\end{equation}
where $g_i(a,b) = (a^\top x^i + b)^2$.   

Notice that the objective function $G(a,b) = \sum_{i=1}^N g_i(a,b)$  is a convex quadratic function.  The gradient of the objective function is 
$$
\nabla G(a,b) = \sum_{i=1}^N \nabla g_i(a,b) = \sum_{i=1}^N 2x^i (a^\top x^i + b)
$$

Hence, if we want to use gradient descent, we must compute this large sum (think of $N \approx 10,000$).  

Instead, we can \textbf{approximate the gradient!}.  Let $\tilde \nabla G(a,b)$ be our approximate gradient.  We will compute this by randomly choosing a value $r \in \{1, \dots, N\}$ (with uniform probability).  Then set

$$
\tilde \nabla G(a,b) = \nabla g_r(a,b).
$$

It holds that the expected value is the same as the gradient, that is,

$$
\mathbb{E}(\tilde \nabla G(a,b)) = G(a,b).
$$


Hence, we can make probabilistic arguments that these two will have the same (or similar) convergence properties (in expectation).
\end{example}

\subsection{Neural Networks}

\begin{resource}
\begin{itemize}
\item \href{https://www.youtube.com/watch?v=KNAWp2S3w94&feature=emb_logo&ab_channel=TensorFlow}{ML Zero to Hero - Part 1:Intro to machine learning}
\item \href{https://www.youtube.com/watch?v=bemDFpNooA8&ab_channel=TensorFlow}{ML Zero to Hero - Part 2: Basic Computer Vision with ML}
\end{itemize}
\end{resource}



\subsection{Choice of $\Delta_k$ using the hessian $\nabla^2 f(x)$}
These choices are called \emph{second order methods}
\begin{enumerate}
\item \textbf{Newton's Method:}   $\Delta_k = - (\nabla^2f(x_k))^{-1} \nabla f(x_k)$
\item \textbf{BFGS (Quasi-Newton):} $\Delta_k = - (B_k)^{-1} \nabla f(x_k)$
\end{enumerate}
Here
\begin{align*}
s_k &= x_{k+1} - x_k\\
y_k &= \nabla f(x_{k+1}) - \nabla f(x_k)
\end{align*}
and
$$
B_{k+1} = B_k - \frac{(B_k s_k)(B_k s_k)^\top}{s_k^\top B_k s_k} + \frac{y_k y_k^\top}{y_k^\top s_k}.
$$
This serves as an approximation of the hessian and can be efficiently computed.  Furthermore,  the inverse can be easily computed using certain updating rules.  This makes for a fast way to approximate the hessian.



\section{Constrained Convex Nonlinear Programming}
Given a convex function $f(x)\colon \R^d \to \R$ and convex functions $f_i(x)\colon \R^d \to \R$ for $i=1, \dots, m$,  the \emph{convex programming} problem is
\begin{equation}
\label{eq:convex-programming-for-example}
\begin{split}
\min \quad & f(x)\\
\st  \quad & f_i(x) \leq 0  \quad  \text{ for } i=1, \dots, m\\
& x \in \R^d
\end{split}
\end{equation}

\subsection{Barrier Method}
\begin{general}{Constrained Convex Programming via Barrier Method}{}
We convert~\ref{eq:convex-programming-for-example} into the unconstrained minimization problem:
\begin{equation}
\label{eq:convex-programming-barrier}
\begin{split}
\min \quad & f(x) - \phi \sum_{i=1}^m \log(-f_i(x)) \\
& x \in \R^d
\end{split}
\end{equation}
Here $\phi > 0$ is some number that we choose.  As $\phi \to 0$, the optimal solution $x(\phi)$ to \eqref{eq:convex-programming-barrier} tends to the optimal solution of \eqref{eq:convex-programming-for-example}.  That is $x(\phi) \to x^*$ as $\phi \to 0$.
\end{general}

\begin{general}{Constrained Convex Programming via Barrier Method - Initial solution}{}
Define a variable $s \in \R$ and add that to the right hand side of the inequalities and then minimize it in the objective function.
\begin{equation}
\label{eq:convex-programming}
\begin{split}
\min \quad & s\\
\st  \quad & f_i(x) \leq s \quad  \text{ for } i=1, \dots, m\\
& x \in \R^d, s \in \R
\end{split}
\end{equation}
Note that this problem is feasible for all $x$ values since $s$ can always be made larger.  
If there exists a solution with $s \leq 0$, then we can use the corresponding $x$ solution as an initial feasible solution.  Otherwise, the problem is infeasible.

Now, convert this problem into the unconstrained minimization problem:
\begin{equation}
\label{eq:convex-programming}
\begin{split}
\min \quad & f(x) - \phi \sum_{i=1}^m \log(-(f_i(x)-s)) \\
& x \in \R^d, s \in \R
\end{split}
\end{equation}
This problem has an easy time of finding an initial feasible solution.  For instance, let $x = 0$, and then $s = \max_i f_i(x) +1 $.  
\end{general}


\textbf{Images below: the value $t$ is the value $\phi$ discussed above}
\begin{figure}
\includegraphics[width = 0.35\textwidth]{barrier-function}\ \ 
\includegraphics[width = 0.35\textwidth]{barrier-functions-added}\\
\includegraphics[width = 0.35\textwidth]{barrier-method} \ \ 
\includegraphics[width = 0.35\textwidth]{central-path1}
\caption{Upper Left: Barrier function for varying values of $t$.  Upper Right: Combining barrier functions to create a barrier function on the interval $1 \leq x \leq 2$. Lower left: Combining barrier function and the objective function $x^2$ for cetain choices of $t$.  Lower right: Same picture but more values of $t$.}
\end{figure}



\begin{figure}
\includegraphics[scale = 0.35]{interior-point-method} \ \ 
\includegraphics[scale = 0.35]{interior-mu-1-1}

\caption{Barrier method appled to minimizing a linear program with two variables.  Contours of the barrier plotted and the red curve is an approximation of the central path.}
\end{figure}

\textbf{Complexity}
11.5.3 Total number of Newton iterations
We can now give an upper bound on the total number of Newton steps in the barrier method, not counting the initial centering step (which we will analyze later, as part of phase I). We multiply (11.26), which bounds the number of Newton steps per outer iteration, by (11.13), the number of outer steps required, to obtain

$$
N=\left\lceil\frac{\log \left(m /\left(t^{(0)} \epsilon\right)\right)}{\log \mu}\right\rceil\left(\frac{m(\mu-1-\log \mu)}{\gamma}+c\right)
$$



\chapter{Computational Issues with NLP}
\todoChapter{ 50\% complete. Goal 80\% completion date: November 20\\
Notes: }
We mention a few computational issues to consider with nonlinear programs.  
\section{Irrational Solutions}
Consider nonlinear problem (this  is even convex)
\begin{equation}
\begin{array}{rl}
\min \quad & -x\\
\text{s.t.} \quad & x^2 \leq 2.
\end{array}
\end{equation}
The optimal solution is $x^* = \sqrt{2}$, which cannot be easily represented.  Hence, we would settle for an \textbf{approximate solution} such as $\bar x = 1.41421$, which is feasible since $\bar x^2 \leq 2$, and it is close to optimal.

\section{Discrete Solutions}
Consider nonlinear problem (not convex)
\begin{equation}
\begin{array}{rl}
\min \quad & -x\\
\text{s.t.} \quad & x^2 = 2.
\end{array}
\end{equation}
Just as before, the optimal solution is $x^* = \sqrt{2}$, which cannot be easily represented.  Furthermore, the only two feasible solutions are $\sqrt{2}$ and $-\sqrt{2}$.  Thus, there is no chance to write down a feasible rational approximation.

\section{Convex NLP Harder than LP}
Convex NLP is typically polynomially solvable.  It is a generalization of linear programming.  
\begin{general}{Convex Programming}{\polynomial\ \  (typically)}
Given a convex function $f(x)\colon \R^d \to \R$ and convex functions $f_i(x)\colon \R^d \to \R$ for $i=1, \dots, m$,  the \emph{convex programming} problem is
\begin{equation}
\label{eq:convex-programming}
\begin{split}
\min \quad & f(x)\\
\st  \quad & f_i(x) \leq 0  \quad  \text{ for } i=1, \dots, m\\
& x \in \R^d
\end{split}
\end{equation}
\end{general}
\begin{example}
Convex programming is a generalization of linear programming.  This can be seen by letting $f(x) = c^\top x$ and $f_i(x) = A_i x - b_i$.  
\end{example}


\section{NLP is harder than IP}
As seen above, quadratic constraints can be used to create a feasible region with discrete solutions.  For example 
$$
x(1-x) = 0
$$
has exactly two solutions: $x = 0, x=1$.  
Thus, quadratic constraints can be used to model binary constraints.
\begin{general}{Binary Integer programming (BIP) as a NLP}{\nphard}
Given a matrix $A \in \R^{m\times n}$, vector $b \in \R^m$ and vector $c \in \R^n$, the \emph{binary integer programming} problem is
\begin{equation}
\label{eq:BIP}
\begin{split}
\max \quad & c^\top x\\
\st  \quad & Ax \leq b\\
& \hcancel[1.5pt]{x \in \{0,1\}^n}\\
& x_i(1-x_i) = 0 \quad \text{ for } i=1, \dots, n
\end{split}
\end{equation}
\end{general}


 







\section{Resources}


\begin{resource}
\paragraph{Gradient Free Algorithms - Needler-Mead}
\begin{itemize}
\item \href{https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method}{Wikipedia}\\
\item \href{https://youtube/NI3WllrvWoc?t=96}{Youtube}
\end{itemize}
\paragraph{Bisection Method and Newton's Method}
\begin{itemize}
\item See section 4 of the following notes:
\url{http://www.seas.ucla.edu/~vandenbe/133A/133A-notes.pdf}
\end{itemize}
\paragraph{Gradient Descent}
\begin{itemize}
\item \url{https://www.youtube.com/watch?v=tIpKfDc295M}

\item \url{https://www.youtube.com/watch?v=_-02ze7tf08}

\item \url{https://www.youtube.com/watch?v=N_ZRcLheNv0}

\item \url{https://www.youtube.com/watch?v=4RBkIJPG6Yo}
\end{itemize}
\paragraph{Idea of Gradient descent}
\begin{itemize}
\item \url{https://youtu.be/IHZwWFHWa-w?t=323}
\end{itemize}
Vectors:\\
\begin{itemize}
\item \url{https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=2&t=0s}
\end{itemize}
\end{resource}






\chapter{Fairness in Algorithms}
\todoChapter{ Decide if we want to include this chapter or not.  No material currently written for it. }
\begin{resource}
\begin{itemize}
\item \href{https://www.youtube.com/watch?v=jtZdytJA0m8&ab_channel=SimonsInstitute}{Simons Institute - Michael Kearns (University of Pennsylvania) - "The Ethical Algorithm"}
\end{itemize}
\end{resource}



%
%\end{document}
