\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\pbs@newkey[2]{}\providecommand\pbs@seq@push@cx[2]{}\providecommand\pbs@at@end@dvi@check{}
\pbs@at@end@dvi@check
\providecommand\@anim@newkey[2]{}
\pbs@newkey{pbs@last@page}{1}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}NLP Algorithms}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lopytx}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Algorithms Introduction}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}1-Dimensional Algorithms}{1}{section.1.2}}
\pbs@newkey{pbs@last@page}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Golden Search Method - Derivative Free Algorithm}{2}{subsection.1.2.1}}
\pbs@newkey{pbs@last@page}{3}
\pbs@newkey{pbs@last@page}{4}
\@writefile{toc}{\contentsline {subsubsection}{Example:}{4}{section*.1}}
\pbs@newkey{pbs@last@page}{5}
\pbs@newkey{pbs@last@page}{6}
\pbs@newkey{pbs@last@page}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Bisection Method - 1st Order Method (using Derivative)}{7}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Minimization Interpretation}{7}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Root finding Interpretation}{7}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Gradient Descent - 1st Order Method (using Derivative)}{7}{subsection.1.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Newton's Method - 2nd Order Method (using Derivative and Hessian)}{7}{subsection.1.2.4}}
\pbs@newkey{pbs@last@page}{8}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Multi-Variate Unconstrained Optimizaition}{8}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Descent Methods - Unconstrained Optimization - Gradient, Newton}{8}{subsection.1.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of $\alpha _t$}{8}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of $d_t$ using $\nabla f(x)$}{8}{section*.5}}
\pbs@newkey{pbs@last@page}{9}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Stochastic Gradient Descent - The mother of all algorithms.}{9}{subsection.1.3.2}}
\pbs@newkey{pbs@last@page}{10}
\@writefile{toc}{\contentsline {subsubsection}{Choice of $\Delta _k$ using the hessian $\nabla ^2 f(x)$}{10}{section*.6}}
\pbs@newkey{pbs@last@page}{11}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Constrained Convex Nonlinear Programming}{11}{section.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Barrier Method}{11}{section.1.5}}
\newlabel{eq:convex-programming-for-example}{{1.5.1}{11}{Constrained Convex Programming via Barrier Method}{equation.1.5.1}{}}
\newlabel{eq:convex-programming-barrier}{{1.5.2}{11}{Constrained Convex Programming via Barrier Method}{equation.1.5.2}{}}
\newlabel{eq:convex-programming}{{1.5.3}{11}{Constrained Convex Programming via Barrier Method - Initial solution}{equation.1.5.3}{}}
\newlabel{eq:convex-programming}{{1.5.4}{11}{Constrained Convex Programming via Barrier Method - Initial solution}{equation.1.5.4}{}}
\pbs@newkey{pbs@last@page}{12}
\pbs@newkey{pbs@last@page}{13}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Computational Issues with NLP}{13}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lopytx}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Irrational Solutions}{13}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Discrete Solutions}{13}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convex NLP Harder than LP}{13}{section.2.3}}
\pbs@newkey{pbs@last@page}{14}
\newlabel{eq:convex-programming}{{2.3.1}{14}{Convex Programming}{equation.2.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}NLP is harder than IP}{14}{section.2.4}}
\newlabel{eq:BIP}{{2.4.1}{14}{Binary Integer programming (BIP) as a NLP}{equation.2.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Karush-Huhn-Tucker (KKT) Conditions}{14}{section.2.5}}
\pbs@newkey{pbs@last@page}{15}
\newlabel{eq:convex-programming-KKT}{{2.5.1}{15}{KKT Conditions for Optimality}{equation.2.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Gradient Free Algorithms}{15}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Needler-Mead}{15}{subsection.2.6.1}}
\pbs@newkey{pbs@last@page}{16}
\pbs@newkey{pbs@last@page}{17}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Material to add...}{17}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lopytx}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.1}Bisection Method and Newton's Method}{17}{subsection.3.0.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gradient Descent}{17}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Projection Gradient Methods}{17}{section.3.2}}
