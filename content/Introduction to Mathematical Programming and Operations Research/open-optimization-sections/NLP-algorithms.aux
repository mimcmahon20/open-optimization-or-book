\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\pbs@newkey[2]{}\providecommand\pbs@seq@push@cx[2]{}\providecommand\pbs@at@end@dvi@check{}
\pbs@at@end@dvi@check
\providecommand\@anim@newkey[2]{}
\pbs@newkey{pbs@last@page}{1}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}NLP Algorithms}{1}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lopytx}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Algorithms Introduction}{1}{section.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}1-Dimensional Algorithms}{1}{section.1.2}}
\pbs@newkey{pbs@last@page}{2}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Golden Search Method - Derivative Free Algorithm}{2}{subsection.1.2.1}}
\pbs@newkey{pbs@last@page}{3}
\@writefile{toc}{\contentsline {subsubsection}{Example:}{3}{section*.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Bisection Method - 1st Order Method (using Derivative)}{3}{subsection.1.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{Minimization Interpretation}{3}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Root finding Interpretation}{3}{section*.3}}
\pbs@newkey{pbs@last@page}{4}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Gradient Descent - 1st Order Method (using Derivative)}{4}{subsection.1.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Newton's Method - 2nd Order Method (using Derivative and Hessian)}{4}{subsection.1.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Multi-Variate Unconstrained Optimizaition}{4}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Descent Methods - Unconstrained Optimization - Gradient, Newton}{4}{subsection.1.3.1}}
\pbs@newkey{pbs@last@page}{5}
\@writefile{toc}{\contentsline {subsubsection}{Choice of $\alpha _t$}{5}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{Choice of $d_t$ using $\nabla f(x)$}{5}{section*.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Stochastic Gradient Descent - The mother of all algorithms.}{5}{subsection.1.3.2}}
\pbs@newkey{pbs@last@page}{6}
\pbs@newkey{pbs@last@page}{7}
\@writefile{toc}{\contentsline {subsubsection}{Choice of $\Delta _k$ using the hessian $\nabla ^2 f(x)$}{7}{section*.6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Constrained Convex Nonlinear Programming}{7}{section.1.4}}
\newlabel{eq:convex-programming-for-example}{{1.4.1}{7}{Constrained Convex Nonlinear Programming}{equation.1.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Barrier Method}{7}{subsection.1.4.1}}
\pbs@newkey{pbs@last@page}{8}
\newlabel{eq:convex-programming-barrier}{{1.4.2}{8}{Constrained Convex Programming via Barrier Method}{equation.1.4.2}{}}
\newlabel{eq:convex-programming}{{1.4.3}{8}{Constrained Convex Programming via Barrier Method - Initial solution}{equation.1.4.3}{}}
\newlabel{eq:convex-programming}{{1.4.4}{8}{Constrained Convex Programming via Barrier Method - Initial solution}{equation.1.4.4}{}}
\pbs@newkey{pbs@last@page}{9}
\pbs@newkey{pbs@last@page}{10}
\pbs@newkey{pbs@last@page}{11}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Computational Issues with NLP}{11}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lopytx}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Irrational Solutions}{11}{section.2.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Discrete Solutions}{11}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Convex NLP Harder than LP}{11}{section.2.3}}
\pbs@newkey{pbs@last@page}{12}
\newlabel{eq:convex-programming}{{2.3.1}{12}{Convex Programming}{equation.2.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}NLP is harder than IP}{12}{section.2.4}}
\newlabel{eq:BIP}{{2.4.1}{12}{Binary Integer programming (BIP) as a NLP}{equation.2.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Karush-Huhn-Tucker (KKT) Conditions}{12}{section.2.5}}
\pbs@newkey{pbs@last@page}{13}
\newlabel{eq:convex-programming-KKT}{{2.5.1}{13}{KKT Conditions for Optimality}{equation.2.5.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Gradient Free Algorithms}{13}{section.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Needler-Mead}{13}{subsection.2.6.1}}
\pbs@newkey{pbs@last@page}{14}
\pbs@newkey{pbs@last@page}{15}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Material to add...}{15}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lopytx}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.0.1}Bisection Method and Newton's Method}{15}{subsection.3.0.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Gradient Descent}{15}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Projection Gradient Methods}{15}{section.3.2}}
