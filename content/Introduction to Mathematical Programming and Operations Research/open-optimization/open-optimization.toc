\contentsline {chapter}{\numberline {1}Introduction}{7}{chapter.1}
\contentsline {section}{\numberline {1.1}Notation}{7}{section.1.1}
\contentsline {part}{I\hspace {1em}Introduction to Optimization}{9}{part.1}
\contentsline {chapter}{\numberline {2}Mathematical Programming}{11}{chapter.2}
\contentsline {section}{\numberline {2.1}Linear Programming (LP)}{11}{section.2.1}
\contentsline {section}{\numberline {2.2}Mixed-Integer Linear Programming (MILP)}{13}{section.2.2}
\contentsline {section}{\numberline {2.3}Non-Linear Programming (NLP)}{14}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}Convex Programming}{15}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}Non-Convex Non-linear Programming}{15}{subsection.2.3.2}
\contentsline {paragraph}{IP as NLP}{15}{section*.5}
\contentsline {section}{\numberline {2.4}Mixed-Integer Non-Linear Programming (MINLP)}{16}{section.2.4}
\contentsline {subsection}{\numberline {2.4.1}Convex Mixed-Integer Non-Linear Programming}{16}{subsection.2.4.1}
\contentsline {subsection}{\numberline {2.4.2}Non-Convex Mixed-Integer Non-Linear Programming}{16}{subsection.2.4.2}
\contentsline {section}{\numberline {2.5}Models}{16}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}INFORMS Studies}{16}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Employee Training Program}{16}{subsection.2.5.2}
\contentsline {subsection}{\numberline {2.5.3}Media Selection Program}{16}{subsection.2.5.3}
\contentsline {subsection}{\numberline {2.5.4}Diet Problem}{16}{subsection.2.5.4}
\contentsline {subsection}{\numberline {2.5.5}Farm Planning Problem}{16}{subsection.2.5.5}
\contentsline {subsection}{\numberline {2.5.6}Pooling Problem}{16}{subsection.2.5.6}
\contentsline {chapter}{\numberline {3}Algorithms and Complexity}{17}{chapter.3}
\contentsline {section}{\numberline {3.1}Big-O Notation}{17}{section.3.1}
\contentsline {section}{\numberline {3.2}Algorithms - Example with Bubble Sort}{20}{section.3.2}
\contentsline {subsection}{\numberline {3.2.1}Sorting }{20}{subsection.3.2.1}
\contentsline {section}{\numberline {3.3}Complexity Classes}{22}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}P}{23}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}NP}{23}{subsection.3.3.2}
\contentsline {subsection}{\numberline {3.3.3}NP-Hard}{24}{subsection.3.3.3}
\contentsline {subsection}{\numberline {3.3.4}NP-Complete}{24}{subsection.3.3.4}
\contentsline {section}{\numberline {3.4}Relevant Terminology}{25}{section.3.4}
\contentsline {section}{\numberline {3.5}Matching Problem}{26}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Greedy Algorithm for Maximal Matching}{27}{subsection.3.5.1}
\contentsline {subsection}{\numberline {3.5.2}Other algorithms to look at}{27}{subsection.3.5.2}
\contentsline {section}{\numberline {3.6}Minimum Spanning Tree}{27}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Kruskal's algorithm}{28}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Prim's Algorithm}{28}{subsection.3.6.2}
\contentsline {section}{\numberline {3.7}Traveling Salesman Problem}{28}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Nearest Neighbor - Construction Heuristic}{29}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Double Spanning Tree - 2-Apx}{29}{subsection.3.7.2}
\contentsline {subsection}{\numberline {3.7.3}Christofides - Approximation Algorithm - $(3/2)$-Apx}{30}{subsection.3.7.3}
\contentsline {chapter}{\numberline {4}Integer Programming Formulations}{31}{chapter.4}
\contentsline {section}{\numberline {4.1}Knapsack Problem}{31}{section.4.1}
\contentsline {section}{\numberline {4.2}Set Covering}{33}{section.4.2}
\contentsline {paragraph}{Sets}{35}{section*.12}
\contentsline {paragraph}{Variables}{35}{section*.13}
\contentsline {paragraph}{Model}{35}{section*.14}
\contentsline {subsubsection}{General Set Covering}{37}{section*.15}
\contentsline {section}{\numberline {4.3}Machine Assignment}{38}{section.4.3}
\contentsline {section}{\numberline {4.4}Facility Location}{39}{section.4.4}
\contentsline {subsection}{\numberline {4.4.1}Capacitated Facility Location}{39}{subsection.4.4.1}
\contentsline {subsection}{\numberline {4.4.2}Uncapacitated Facility Location}{40}{subsection.4.4.2}
\contentsline {section}{\numberline {4.5}Capital Budgeting}{40}{section.4.5}
\contentsline {section}{\numberline {4.6}Network Flow}{41}{section.4.6}
\contentsline {section}{\numberline {4.7}Transportation Problem}{41}{section.4.7}
\contentsline {subsection}{\numberline {4.7.1}Modeling Tricks}{41}{subsection.4.7.1}
\contentsline {subsection}{\numberline {4.7.2}Either Or Constraints}{41}{subsection.4.7.2}
\contentsline {subsubsection}{If then implications}{42}{section*.18}
\contentsline {subsection}{\numberline {4.7.3}Binary reformulation of integer variables}{43}{subsection.4.7.3}
\contentsline {subsection}{\numberline {4.7.4}SOS1 Constraints}{44}{subsection.4.7.4}
\contentsline {subsection}{\numberline {4.7.5}SOS2 Constraints}{45}{subsection.4.7.5}
\contentsline {subsection}{\numberline {4.7.6}Piecewise linear functions with SOS2 constraint}{45}{subsection.4.7.6}
\contentsline {subsection}{\numberline {4.7.7}Maximizing a minimum}{47}{subsection.4.7.7}
\contentsline {subsection}{\numberline {4.7.8}Relaxing (nonlinear) equality constraints}{48}{subsection.4.7.8}
\contentsline {section}{\numberline {4.8}Notes from AIMMS modeling book.}{48}{section.4.8}
\contentsline {subsection}{\numberline {4.8.1}Guidelines for Integer Programming Modeling}{48}{subsection.4.8.1}
\contentsline {subsection}{\numberline {4.8.2}Linear Programming Modeling}{48}{subsection.4.8.2}
\contentsline {subsection}{\numberline {4.8.3}From AIMMS}{48}{subsection.4.8.3}
\contentsline {subsubsection}{Linear Programming}{48}{section*.20}
\contentsline {subsection}{\numberline {4.8.4}Further Topics}{48}{subsection.4.8.4}
\contentsline {subsubsection}{Precedence Constraints}{48}{section*.21}
\contentsline {chapter}{\numberline {5}Exponential Size Integer Programming Formulations}{49}{chapter.5}
\contentsline {section}{\numberline {5.1}Cutting Stock}{49}{section.5.1}
\contentsline {subsection}{\numberline {5.1.1}Pattern formulation}{52}{subsection.5.1.1}
\contentsline {subsection}{\numberline {5.1.2}Column Generation}{53}{subsection.5.1.2}
\contentsline {subsection}{\numberline {5.1.3}Cutting Stock - Multiple widths}{54}{subsection.5.1.3}
\contentsline {section}{\numberline {5.2}Spanning Trees}{55}{section.5.2}
\contentsline {section}{\numberline {5.3}Traveling Salesman Problem}{57}{section.5.3}
\contentsline {paragraph}{Models}{57}{section*.22}
\contentsline {subsubsection}{MTZ Model}{58}{section*.23}
\contentsline {paragraph}{Pros of this model}{62}{section*.24}
\contentsline {paragraph}{Cons of this model}{62}{section*.25}
\contentsline {subsection}{\numberline {5.3.1}Dantzig-Fulkerson-Johnson Model}{62}{subsection.5.3.1}
\contentsline {paragraph}{Pros of this model}{65}{section*.26}
\contentsline {paragraph}{Cons of this model}{65}{section*.27}
\contentsline {paragraph}{Solution: Add subtour elimination constraints as needed. We will discuss this in a future section on \emph {cutting planes}}{65}{section*.28}
\contentsline {subsection}{\numberline {5.3.2}Traveling Salesman Problem - Branching Solution}{65}{subsection.5.3.2}
\contentsline {section}{\numberline {5.4}Google maps data}{65}{section.5.4}
\contentsline {section}{\numberline {5.5}Literature}{65}{section.5.5}
\contentsline {chapter}{\numberline {6}Algorithms to Solve Integer Programs}{67}{chapter.6}
\contentsline {section}{\numberline {6.1}LP to solve IP}{67}{section.6.1}
\contentsline {subsection}{\numberline {6.1.1}Rounding LP Solution can be bad!}{68}{subsection.6.1.1}
\contentsline {subsection}{\numberline {6.1.2}Rounding LP solution can be infeasible!}{68}{subsection.6.1.2}
\contentsline {subsection}{\numberline {6.1.3}Fractional Knapsack}{68}{subsection.6.1.3}
\contentsline {section}{\numberline {6.2}Branch and Bound}{68}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Algorithm}{69}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}General Branching}{69}{subsection.6.2.2}
\contentsline {subsection}{\numberline {6.2.3}Knapsack Problem and 0/1 branching}{77}{subsection.6.2.3}
\contentsline {subsection}{\numberline {6.2.4}Traveling Salesman Problem solution via Branching}{82}{subsection.6.2.4}
\contentsline {section}{\numberline {6.3}Cutting Planes}{82}{section.6.3}
\contentsline {subsection}{\numberline {6.3.1}Chv\'atal Cuts}{83}{subsection.6.3.1}
\contentsline {subsection}{\numberline {6.3.2}Gomory Cuts}{84}{subsection.6.3.2}
\contentsline {subsection}{\numberline {6.3.3}Fun with cutting planes}{91}{subsection.6.3.3}
\contentsline {section}{\numberline {6.4}Branching Rules}{91}{section.6.4}
\contentsline {section}{\numberline {6.5}Lagrangian Relaxation for Branch and Bound}{91}{section.6.5}
\contentsline {section}{\numberline {6.6}Literature}{92}{section.6.6}
\contentsline {chapter}{\numberline {7}Non-linear Programming (NLP)}{95}{chapter.7}
\contentsline {section}{\numberline {7.1}Convex Sets}{96}{section.7.1}
\contentsline {section}{\numberline {7.2}Convex Functions}{97}{section.7.2}
\contentsline {subsection}{\numberline {7.2.1}Proving Convexity - Characterizations}{99}{subsection.7.2.1}
\contentsline {subsection}{\numberline {7.2.2}Proving Convexity - Composition Tricks}{100}{subsection.7.2.2}
\contentsline {section}{\numberline {7.3}Convex Optimization Examples}{101}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}Unconstrained Optimization: Linear Regression}{101}{subsection.7.3.1}
\contentsline {section}{\numberline {7.4}Machine Learning - SVM}{102}{section.7.4}
\contentsline {paragraph}{Input:}{102}{section*.38}
\contentsline {paragraph}{Output:}{103}{section*.39}
\contentsline {subsubsection}{Feasible separation}{103}{section*.40}
\contentsline {subsubsection}{SVM}{103}{section*.41}
\contentsline {subsubsection}{Approximate SVM}{104}{section*.42}
\contentsline {subsection}{\numberline {7.4.1}SVM with non-linear separators}{104}{subsection.7.4.1}
\contentsline {subsection}{\numberline {7.4.2}Support Vector Machines}{105}{subsection.7.4.2}
\contentsline {section}{\numberline {7.5}Classification}{107}{section.7.5}
\contentsline {subsection}{\numberline {7.5.1}Machine Learning}{107}{subsection.7.5.1}
\contentsline {subsection}{\numberline {7.5.2}Neural Networks}{107}{subsection.7.5.2}
\contentsline {section}{\numberline {7.6}Box Volume Optimization in Scipy.Minimize}{107}{section.7.6}
\contentsline {section}{\numberline {7.7}Modeling}{108}{section.7.7}
\contentsline {paragraph}{Important tips}{108}{section*.43}
\contentsline {subsection}{\numberline {7.7.1}Minimum distance to circles}{108}{subsection.7.7.1}
\contentsline {section}{\numberline {7.8}Machine Learning}{111}{section.7.8}
\contentsline {section}{\numberline {7.9}Machine Learning - Supervised Learning - Regression}{111}{section.7.9}
\contentsline {section}{\numberline {7.10}Machine learning - Supervised Learning - Classification}{112}{section.7.10}
\contentsline {subsection}{\numberline {7.10.1}Python SGD implementation and video}{112}{subsection.7.10.1}
\contentsline {chapter}{\numberline {8}NLP Algorithms}{113}{chapter.8}
\contentsline {section}{\numberline {8.1}Algorithms Introduction}{113}{section.8.1}
\contentsline {section}{\numberline {8.2}1-Dimensional Algorithms}{113}{section.8.2}
\contentsline {subsection}{\numberline {8.2.1}Golden Search Method - Derivative Free Algorithm}{114}{subsection.8.2.1}
\contentsline {subsubsection}{Example:}{116}{section*.44}
\contentsline {subsection}{\numberline {8.2.2}Bisection Method - 1st Order Method (using Derivative)}{119}{subsection.8.2.2}
\contentsline {subsubsection}{Minimization Interpretation}{119}{section*.45}
\contentsline {subsubsection}{Root finding Interpretation}{119}{section*.46}
\contentsline {subsection}{\numberline {8.2.3}Gradient Descent - 1st Order Method (using Derivative)}{119}{subsection.8.2.3}
\contentsline {subsection}{\numberline {8.2.4}Newton's Method - 2nd Order Method (using Derivative and Hessian)}{119}{subsection.8.2.4}
\contentsline {section}{\numberline {8.3}Multi-Variate Unconstrained Optimizaition}{120}{section.8.3}
\contentsline {subsection}{\numberline {8.3.1}Descent Methods - Unconstrained Optimization - Gradient, Newton}{120}{subsection.8.3.1}
\contentsline {subsubsection}{Choice of $\alpha _t$}{120}{section*.47}
\contentsline {subsubsection}{Choice of $d_t$ using $\nabla f(x)$}{120}{section*.48}
\contentsline {subsection}{\numberline {8.3.2}Stochastic Gradient Descent - The mother of all algorithms.}{121}{subsection.8.3.2}
\contentsline {subsubsection}{Choice of $\Delta _k$ using the hessian $\nabla ^2 f(x)$}{122}{section*.49}
\contentsline {section}{\numberline {8.4}Constrained Convex Nonlinear Programming}{123}{section.8.4}
\contentsline {subsection}{\numberline {8.4.1}Barrier Method}{123}{subsection.8.4.1}
\contentsline {chapter}{\numberline {9}Computational Issues with NLP}{125}{chapter.9}
\contentsline {section}{\numberline {9.1}Irrational Solutions}{125}{section.9.1}
\contentsline {section}{\numberline {9.2}Discrete Solutions}{125}{section.9.2}
\contentsline {section}{\numberline {9.3}Convex NLP Harder than LP}{125}{section.9.3}
\contentsline {section}{\numberline {9.4}NLP is harder than IP}{126}{section.9.4}
\contentsline {section}{\numberline {9.5}Karush-Huhn-Tucker (KKT) Conditions}{126}{section.9.5}
\contentsline {section}{\numberline {9.6}Gradient Free Algorithms}{127}{section.9.6}
\contentsline {subsection}{\numberline {9.6.1}Needler-Mead}{127}{subsection.9.6.1}
\contentsline {chapter}{\numberline {10}Material to add...}{129}{chapter.10}
\contentsline {subsection}{\numberline {10.0.1}Bisection Method and Newton's Method}{129}{subsection.10.0.1}
\contentsline {section}{\numberline {10.1}Gradient Descent}{129}{section.10.1}
\contentsline {section}{\numberline {10.2}Projection Gradient Methods}{129}{section.10.2}
