\contentsline {section}{\numberline {0.1}Notation}{7}{section.0.1}
\contentsline {part}{I\hspace {1em}Introduction to Optimization}{9}{part.1}
\contentsline {chapter}{\numberline {1}Mathematical Programming}{11}{chapter.1}
\contentsline {section}{\numberline {1.1}Linear Programming (LP)}{11}{section.1.1}
\contentsline {section}{\numberline {1.2}Mixed-Integer Linear Programming (MILP)}{13}{section.1.2}
\contentsline {section}{\numberline {1.3}Non-Linear Programming (NLP)}{14}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Convex Programming}{15}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Non-Convex Non-linear Programming}{15}{subsection.1.3.2}
\contentsline {paragraph}{IP as NLP}{15}{section*.6}
\contentsline {section}{\numberline {1.4}Mixed-Integer Non-Linear Programming (MINLP)}{16}{section.1.4}
\contentsline {subsection}{\numberline {1.4.1}Convex Mixed-Integer Non-Linear Programming}{16}{subsection.1.4.1}
\contentsline {subsection}{\numberline {1.4.2}Non-Convex Mixed-Integer Non-Linear Programming}{16}{subsection.1.4.2}
\contentsline {section}{\numberline {1.5}Models}{16}{section.1.5}
\contentsline {subsection}{\numberline {1.5.1}INFORMS Studies}{16}{subsection.1.5.1}
\contentsline {subsection}{\numberline {1.5.2}Employee Training Program}{16}{subsection.1.5.2}
\contentsline {subsection}{\numberline {1.5.3}Media Selection Program}{16}{subsection.1.5.3}
\contentsline {subsection}{\numberline {1.5.4}Diet Problem}{16}{subsection.1.5.4}
\contentsline {subsection}{\numberline {1.5.5}Farm Planning Problem}{16}{subsection.1.5.5}
\contentsline {subsection}{\numberline {1.5.6}Pooling Problem}{16}{subsection.1.5.6}
\contentsline {chapter}{\numberline {2}Algorithms and Complexity}{17}{chapter.2}
\contentsline {section}{\numberline {2.1}Big-O Notation}{17}{section.2.1}
\contentsline {section}{\numberline {2.2}Algorithms - Example with Bubble Sort}{20}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Sorting }{20}{subsection.2.2.1}
\contentsline {section}{\numberline {2.3}Complexity Classes}{22}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}P}{23}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}NP}{23}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}NP-Hard}{24}{subsection.2.3.3}
\contentsline {subsection}{\numberline {2.3.4}NP-Complete}{24}{subsection.2.3.4}
\contentsline {section}{\numberline {2.4}Relevant Terminology}{25}{section.2.4}
\contentsline {section}{\numberline {2.5}Matching Problem}{26}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Greedy Algorithm for Maximal Matching}{26}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Other algorithms to look at}{27}{subsection.2.5.2}
\contentsline {section}{\numberline {2.6}Minimum Spanning Tree}{27}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Kruskal's algorithm}{28}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Prim's Algorithm}{28}{subsection.2.6.2}
\contentsline {section}{\numberline {2.7}Traveling Salesman Problem}{28}{section.2.7}
\contentsline {subsection}{\numberline {2.7.1}Nearest Neighbor - Construction Heuristic}{28}{subsection.2.7.1}
\contentsline {subsection}{\numberline {2.7.2}Double Spanning Tree - 2-Apx}{29}{subsection.2.7.2}
\contentsline {subsection}{\numberline {2.7.3}Christofides - Approximation Algorithm - $(3/2)$-Apx}{30}{subsection.2.7.3}
\contentsline {chapter}{\numberline {3}Integer Programming Formulations}{31}{chapter.3}
\contentsline {section}{\numberline {3.1}Knapsack Problem}{31}{section.3.1}
\contentsline {section}{\numberline {3.2}Set Covering}{33}{section.3.2}
\contentsline {paragraph}{Sets}{35}{section*.13}
\contentsline {paragraph}{Variables}{35}{section*.14}
\contentsline {paragraph}{Model}{35}{section*.15}
\contentsline {subsubsection}{General Set Covering}{37}{section*.16}
\contentsline {section}{\numberline {3.3}Machine Assignment}{38}{section.3.3}
\contentsline {section}{\numberline {3.4}Facility Location}{39}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Capacitated Facility Location}{39}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Uncapacitated Facility Location}{39}{subsection.3.4.2}
\contentsline {section}{\numberline {3.5}Capital Budgeting}{39}{section.3.5}
\contentsline {section}{\numberline {3.6}Network Flow}{40}{section.3.6}
\contentsline {section}{\numberline {3.7}Transportation Problem}{40}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Modeling Tricks}{40}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Either Or Constraints}{40}{subsection.3.7.2}
\contentsline {subsubsection}{If then implications}{41}{section*.19}
\contentsline {subsection}{\numberline {3.7.3}Binary reformulation of integer variables}{43}{subsection.3.7.3}
\contentsline {subsection}{\numberline {3.7.4}SOS1 Constraints}{44}{subsection.3.7.4}
\contentsline {subsection}{\numberline {3.7.5}SOS2 Constraints}{44}{subsection.3.7.5}
\contentsline {subsection}{\numberline {3.7.6}Piecewise linear functions with SOS2 constraint}{45}{subsection.3.7.6}
\contentsline {subsection}{\numberline {3.7.7}Maximizing a minimum}{47}{subsection.3.7.7}
\contentsline {subsection}{\numberline {3.7.8}Relaxing (nonlinear) equality constraints}{48}{subsection.3.7.8}
\contentsline {section}{\numberline {3.8}Notes from AIMMS modeling book.}{48}{section.3.8}
\contentsline {subsection}{\numberline {3.8.1}Guidelines for Integer Programming Modeling}{48}{subsection.3.8.1}
\contentsline {subsection}{\numberline {3.8.2}Linear Programming Modeling}{48}{subsection.3.8.2}
\contentsline {subsection}{\numberline {3.8.3}From AIMMS}{48}{subsection.3.8.3}
\contentsline {subsubsection}{Linear Programming}{48}{section*.21}
\contentsline {subsection}{\numberline {3.8.4}Further Topics}{48}{subsection.3.8.4}
\contentsline {subsubsection}{Precedence Constraints}{48}{section*.22}
\contentsline {chapter}{\numberline {4}Exponential Size Integer Programming Formulations}{49}{chapter.4}
\contentsline {section}{\numberline {4.1}Cutting Stock}{49}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Pattern formulation}{51}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Column Generation}{52}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}Cutting Stock - Multiple widths}{53}{subsection.4.1.3}
\contentsline {section}{\numberline {4.2}Spanning Trees}{53}{section.4.2}
\contentsline {section}{\numberline {4.3}Traveling Salesman Problem}{54}{section.4.3}
\contentsline {paragraph}{Models}{54}{section*.23}
\contentsline {subsubsection}{MTZ Model}{55}{section*.24}
\contentsline {paragraph}{Pros of this model}{59}{section*.25}
\contentsline {paragraph}{Cons of this model}{59}{section*.26}
\contentsline {subsection}{\numberline {4.3.1}Dantzig-Fulkerson-Johnson Model}{59}{subsection.4.3.1}
\contentsline {paragraph}{Pros of this model}{62}{section*.27}
\contentsline {paragraph}{Cons of this model}{62}{section*.28}
\contentsline {paragraph}{Solution: Add subtour elimination constraints as needed. We will discuss this in a future section on \emph {cutting planes}}{62}{section*.29}
\contentsline {subsection}{\numberline {4.3.2}Traveling Salesman Problem - Branching Solution}{62}{subsection.4.3.2}
\contentsline {section}{\numberline {4.4}Google maps data}{62}{section.4.4}
\contentsline {section}{\numberline {4.5}Literature}{62}{section.4.5}
\contentsline {chapter}{\numberline {5}Algorithms to Solve Integer Programs}{63}{chapter.5}
\contentsline {section}{\numberline {5.1}LP to solve IP}{63}{section.5.1}
\contentsline {subsection}{\numberline {5.1.1}Rounding LP Solution can be bad!}{63}{subsection.5.1.1}
\contentsline {subsection}{\numberline {5.1.2}Rounding LP solution can be infeasible!}{64}{subsection.5.1.2}
\contentsline {subsection}{\numberline {5.1.3}Fractional Knapsack}{64}{subsection.5.1.3}
\contentsline {section}{\numberline {5.2}Branch and Bound}{64}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Algorithm}{64}{subsection.5.2.1}
\contentsline {subsection}{\numberline {5.2.2}General Branching}{65}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}Knapsack Problem and 0/1 branching}{68}{subsection.5.2.3}
\contentsline {subsection}{\numberline {5.2.4}Traveling Salesman Problem solution via Branching}{71}{subsection.5.2.4}
\contentsline {section}{\numberline {5.3}Cutting Planes}{71}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Chv\'atal Cuts}{72}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Gomory Cuts}{73}{subsection.5.3.2}
\contentsline {section}{\numberline {5.4}Branching Rules}{75}{section.5.4}
\contentsline {section}{\numberline {5.5}Lagrangian Relaxation for Branch and Bound}{75}{section.5.5}
\contentsline {section}{\numberline {5.6}Literature}{75}{section.5.6}
\contentsline {chapter}{\numberline {6}Non-linear Programming (NLP)}{77}{chapter.6}
\contentsline {section}{\numberline {6.1}Convex Sets}{78}{section.6.1}
\contentsline {section}{\numberline {6.2}Convex Functions}{79}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Proving Convexity - Characterizations}{81}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}Proving Convexity - Composition Tricks}{82}{subsection.6.2.2}
\contentsline {section}{\numberline {6.3}Convex Optimization Examples}{83}{section.6.3}
\contentsline {subsection}{\numberline {6.3.1}Unconstrained Optimization: Linear Regression}{83}{subsection.6.3.1}
\contentsline {section}{\numberline {6.4}Machine Learning - SVM}{84}{section.6.4}
\contentsline {paragraph}{Input:}{84}{section*.39}
\contentsline {paragraph}{Output:}{85}{section*.40}
\contentsline {subsubsection}{Feasible separation}{85}{section*.41}
\contentsline {subsubsection}{SVM}{85}{section*.42}
\contentsline {subsubsection}{Approximate SVM}{86}{section*.43}
\contentsline {subsection}{\numberline {6.4.1}SVM with non-linear separators}{86}{subsection.6.4.1}
\contentsline {subsection}{\numberline {6.4.2}Support Vector Machines}{87}{subsection.6.4.2}
\contentsline {section}{\numberline {6.5}Classification}{88}{section.6.5}
\contentsline {subsection}{\numberline {6.5.1}Machine Learning}{88}{subsection.6.5.1}
\contentsline {subsection}{\numberline {6.5.2}Neural Networks}{88}{subsection.6.5.2}
\contentsline {section}{\numberline {6.6}Box Volume Optimization in Scipy.Minimize}{89}{section.6.6}
\contentsline {section}{\numberline {6.7}Modeling}{89}{section.6.7}
\contentsline {paragraph}{Important tips}{89}{section*.44}
\contentsline {subsection}{\numberline {6.7.1}Minimum distance to circles}{90}{subsection.6.7.1}
\contentsline {section}{\numberline {6.8}Machine Learning}{92}{section.6.8}
\contentsline {section}{\numberline {6.9}Machine Learning - Supervised Learning - Regression}{93}{section.6.9}
\contentsline {section}{\numberline {6.10}Machine learning - Supervised Learning - Classification}{93}{section.6.10}
\contentsline {subsection}{\numberline {6.10.1}Python SGD implementation and video}{93}{subsection.6.10.1}
\contentsline {chapter}{\numberline {7}NLP Algorithms}{95}{chapter.7}
\contentsline {section}{\numberline {7.1}Algorithms Introduction}{95}{section.7.1}
\contentsline {section}{\numberline {7.2}1-Dimensional Algorithms}{95}{section.7.2}
\contentsline {subsection}{\numberline {7.2.1}Golden Search Method - Derivative Free Algorithm}{96}{subsection.7.2.1}
\contentsline {subsubsection}{Example:}{97}{section*.45}
\contentsline {subsection}{\numberline {7.2.2}Bisection Method - 1st Order Method (using Derivative)}{97}{subsection.7.2.2}
\contentsline {subsubsection}{Minimization Interpretation}{97}{section*.46}
\contentsline {subsubsection}{Root finding Interpretation}{97}{section*.47}
\contentsline {subsection}{\numberline {7.2.3}Gradient Descent - 1st Order Method (using Derivative)}{98}{subsection.7.2.3}
\contentsline {subsection}{\numberline {7.2.4}Newton's Method - 2nd Order Method (using Derivative and Hessian)}{98}{subsection.7.2.4}
\contentsline {section}{\numberline {7.3}Multi-Variate Unconstrained Optimizaition}{98}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}Descent Methods - Unconstrained Optimization - Gradient, Newton}{98}{subsection.7.3.1}
\contentsline {subsubsection}{Choice of $\alpha _t$}{99}{section*.48}
\contentsline {subsubsection}{Choice of $d_t$ using $\nabla f(x)$}{99}{section*.49}
\contentsline {subsection}{\numberline {7.3.2}Stochastic Gradient Descent - The mother of all algorithms.}{99}{subsection.7.3.2}
\contentsline {subsubsection}{Choice of $\Delta _k$ using the hessian $\nabla ^2 f(x)$}{101}{section*.50}
\contentsline {section}{\numberline {7.4}Constrained Convex Nonlinear Programming}{101}{section.7.4}
\contentsline {subsection}{\numberline {7.4.1}Barrier Method}{101}{subsection.7.4.1}
\contentsline {chapter}{\numberline {8}Computational Issues with NLP}{105}{chapter.8}
\contentsline {section}{\numberline {8.1}Irrational Solutions}{105}{section.8.1}
\contentsline {section}{\numberline {8.2}Discrete Solutions}{105}{section.8.2}
\contentsline {section}{\numberline {8.3}Convex NLP Harder than LP}{105}{section.8.3}
\contentsline {section}{\numberline {8.4}NLP is harder than IP}{106}{section.8.4}
\contentsline {section}{\numberline {8.5}Karush-Huhn-Tucker (KKT) Conditions}{106}{section.8.5}
\contentsline {section}{\numberline {8.6}Gradient Free Algorithms}{107}{section.8.6}
\contentsline {subsection}{\numberline {8.6.1}Needler-Mead}{107}{subsection.8.6.1}
\contentsline {chapter}{\numberline {9}Material to add...}{109}{chapter.9}
\contentsline {subsection}{\numberline {9.0.1}Bisection Method and Newton's Method}{109}{subsection.9.0.1}
\contentsline {section}{\numberline {9.1}Gradient Descent}{109}{section.9.1}
\contentsline {section}{\numberline {9.2}Projection Gradient Methods}{109}{section.9.2}
