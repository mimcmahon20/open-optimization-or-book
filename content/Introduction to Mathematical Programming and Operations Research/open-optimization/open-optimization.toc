\contentsline {section}{\numberline {0.1}Notation}{5}{section.0.1}
\contentsline {part}{I\hspace {1em}Introduction to Optimization}{7}{part.1}
\contentsline {chapter}{\numberline {1}Mathematical Programming}{9}{chapter.1}
\contentsline {section}{\numberline {1.1}Linear Programming (LP)}{9}{section.1.1}
\contentsline {section}{\numberline {1.2}Mixed-Integer Linear Programming (MILP)}{11}{section.1.2}
\contentsline {section}{\numberline {1.3}Non-Linear Programming (NLP)}{12}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Convex Programming}{13}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Non-Convex Non-linear Programming}{13}{subsection.1.3.2}
\contentsline {paragraph}{IP as NLP}{13}{section*.6}
\contentsline {section}{\numberline {1.4}Mixed-Integer Non-Linear Programming (MINLP)}{14}{section.1.4}
\contentsline {subsection}{\numberline {1.4.1}Convex Mixed-Integer Non-Linear Programming}{14}{subsection.1.4.1}
\contentsline {subsection}{\numberline {1.4.2}Non-Convex Mixed-Integer Non-Linear Programming}{14}{subsection.1.4.2}
\contentsline {section}{\numberline {1.5}Models}{14}{section.1.5}
\contentsline {subsection}{\numberline {1.5.1}INFORMS Studies}{14}{subsection.1.5.1}
\contentsline {subsection}{\numberline {1.5.2}Employee Training Program}{14}{subsection.1.5.2}
\contentsline {subsection}{\numberline {1.5.3}Media Selection Program}{14}{subsection.1.5.3}
\contentsline {subsection}{\numberline {1.5.4}Diet Problem}{14}{subsection.1.5.4}
\contentsline {subsection}{\numberline {1.5.5}Farm Planning Problem}{14}{subsection.1.5.5}
\contentsline {subsection}{\numberline {1.5.6}Pooling Problem}{14}{subsection.1.5.6}
\contentsline {chapter}{\numberline {2}Algorithms and Complexity}{15}{chapter.2}
\contentsline {section}{\numberline {2.1}Big-O Notation}{15}{section.2.1}
\contentsline {section}{\numberline {2.2}Algorithms - Example with Bubble Sort}{18}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Sorting }{18}{subsection.2.2.1}
\contentsline {section}{\numberline {2.3}Complexity Classes}{20}{section.2.3}
\contentsline {subsection}{\numberline {2.3.1}P}{21}{subsection.2.3.1}
\contentsline {subsection}{\numberline {2.3.2}NP}{21}{subsection.2.3.2}
\contentsline {subsection}{\numberline {2.3.3}NP-Hard}{22}{subsection.2.3.3}
\contentsline {subsection}{\numberline {2.3.4}NP-Complete}{22}{subsection.2.3.4}
\contentsline {section}{\numberline {2.4}Relevant Terminology}{23}{section.2.4}
\contentsline {section}{\numberline {2.5}Matching Problem}{24}{section.2.5}
\contentsline {subsection}{\numberline {2.5.1}Greedy Algorithm for Maximal Matching}{24}{subsection.2.5.1}
\contentsline {subsection}{\numberline {2.5.2}Other algorithms to look at}{25}{subsection.2.5.2}
\contentsline {section}{\numberline {2.6}Minimum Spanning Tree}{25}{section.2.6}
\contentsline {subsection}{\numberline {2.6.1}Kruskal's algorithm}{26}{subsection.2.6.1}
\contentsline {subsection}{\numberline {2.6.2}Prim's Algorithm}{26}{subsection.2.6.2}
\contentsline {section}{\numberline {2.7}Traveling Salesman Problem}{26}{section.2.7}
\contentsline {subsection}{\numberline {2.7.1}Nearest Neighbor - Construction Heuristic}{26}{subsection.2.7.1}
\contentsline {subsection}{\numberline {2.7.2}Double Spanning Tree - 2-Apx}{27}{subsection.2.7.2}
\contentsline {subsection}{\numberline {2.7.3}Christofides - Approximation Algorithm - $(3/2)$-Apx}{28}{subsection.2.7.3}
\contentsline {chapter}{\numberline {3}Integer Programming Formulations}{29}{chapter.3}
\contentsline {section}{\numberline {3.1}Knapsack Problem}{29}{section.3.1}
\contentsline {section}{\numberline {3.2}Set Covering}{31}{section.3.2}
\contentsline {paragraph}{Sets}{33}{section*.13}
\contentsline {paragraph}{Variables}{33}{section*.14}
\contentsline {paragraph}{Model}{33}{section*.15}
\contentsline {subsubsection}{General Set Covering}{35}{section*.16}
\contentsline {section}{\numberline {3.3}Machine Assignment}{36}{section.3.3}
\contentsline {section}{\numberline {3.4}Facility Location}{37}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Capacitated Facility Location}{37}{subsection.3.4.1}
\contentsline {subsection}{\numberline {3.4.2}Uncapacitated Facility Location}{37}{subsection.3.4.2}
\contentsline {section}{\numberline {3.5}Capital Budgeting}{37}{section.3.5}
\contentsline {section}{\numberline {3.6}Network Flow}{38}{section.3.6}
\contentsline {section}{\numberline {3.7}Transportation Problem}{38}{section.3.7}
\contentsline {subsection}{\numberline {3.7.1}Modeling Tricks}{38}{subsection.3.7.1}
\contentsline {subsection}{\numberline {3.7.2}Either Or Constraints}{38}{subsection.3.7.2}
\contentsline {subsubsection}{If then implications}{39}{section*.19}
\contentsline {subsection}{\numberline {3.7.3}Binary reformulation of integer variables}{41}{subsection.3.7.3}
\contentsline {subsection}{\numberline {3.7.4}SOS1 Constraints}{42}{subsection.3.7.4}
\contentsline {subsection}{\numberline {3.7.5}SOS2 Constraints}{42}{subsection.3.7.5}
\contentsline {subsection}{\numberline {3.7.6}Piecewise linear functions with SOS2 constraint}{43}{subsection.3.7.6}
\contentsline {subsection}{\numberline {3.7.7}Maximizing a minimum}{45}{subsection.3.7.7}
\contentsline {subsection}{\numberline {3.7.8}Relaxing (nonlinear) equality constraints}{46}{subsection.3.7.8}
\contentsline {section}{\numberline {3.8}Notes from AIMMS modeling book.}{46}{section.3.8}
\contentsline {subsection}{\numberline {3.8.1}Guidelines for Integer Programming Modeling}{46}{subsection.3.8.1}
\contentsline {subsection}{\numberline {3.8.2}Linear Programming Modeling}{46}{subsection.3.8.2}
\contentsline {subsection}{\numberline {3.8.3}From AIMMS}{46}{subsection.3.8.3}
\contentsline {subsubsection}{Linear Programming}{46}{section*.21}
\contentsline {subsection}{\numberline {3.8.4}Further Topics}{46}{subsection.3.8.4}
\contentsline {subsubsection}{Precedence Constraints}{46}{section*.22}
\contentsline {chapter}{\numberline {4}Exponential Size Integer Programming Formulations}{47}{chapter.4}
\contentsline {section}{\numberline {4.1}Cutting Stock}{47}{section.4.1}
\contentsline {subsection}{\numberline {4.1.1}Pattern formulation}{49}{subsection.4.1.1}
\contentsline {subsection}{\numberline {4.1.2}Column Generation}{50}{subsection.4.1.2}
\contentsline {subsection}{\numberline {4.1.3}Cutting Stock - Multiple widths}{51}{subsection.4.1.3}
\contentsline {section}{\numberline {4.2}Spanning Trees}{51}{section.4.2}
\contentsline {section}{\numberline {4.3}Traveling Salesman Problem}{52}{section.4.3}
\contentsline {paragraph}{Models}{52}{section*.23}
\contentsline {subsubsection}{MTZ Model}{53}{section*.24}
\contentsline {paragraph}{Pros of this model}{57}{section*.25}
\contentsline {paragraph}{Cons of this model}{57}{section*.26}
\contentsline {subsection}{\numberline {4.3.1}Dantzig-Fulkerson-Johnson Model}{57}{subsection.4.3.1}
\contentsline {paragraph}{Pros of this model}{60}{section*.27}
\contentsline {paragraph}{Cons of this model}{60}{section*.28}
\contentsline {paragraph}{Solution: Add subtour elimination constraints as needed. We will discuss this in a future section on \emph {cutting planes}}{60}{section*.29}
\contentsline {subsection}{\numberline {4.3.2}Traveling Salesman Problem - Branching Solution}{60}{subsection.4.3.2}
\contentsline {section}{\numberline {4.4}Google maps data}{60}{section.4.4}
\contentsline {section}{\numberline {4.5}Literature}{60}{section.4.5}
\contentsline {chapter}{\numberline {5}Algorithms to Solve Integer Programs}{61}{chapter.5}
\contentsline {section}{\numberline {5.1}LP to solve IP}{61}{section.5.1}
\contentsline {subsection}{\numberline {5.1.1}Rounding LP Solution can be bad!}{61}{subsection.5.1.1}
\contentsline {subsection}{\numberline {5.1.2}Rounding LP solution can be infeasible!}{62}{subsection.5.1.2}
\contentsline {subsection}{\numberline {5.1.3}Fractional Knapsack}{62}{subsection.5.1.3}
\contentsline {section}{\numberline {5.2}Branch and Bound}{62}{section.5.2}
\contentsline {subsection}{\numberline {5.2.1}Algorithm}{62}{subsection.5.2.1}
\contentsline {subsection}{\numberline {5.2.2}General Branching}{63}{subsection.5.2.2}
\contentsline {subsection}{\numberline {5.2.3}Knapsack Problem and 0/1 branching}{66}{subsection.5.2.3}
\contentsline {subsection}{\numberline {5.2.4}Traveling Salesman Problem solution via Branching}{69}{subsection.5.2.4}
\contentsline {section}{\numberline {5.3}Cutting Planes}{69}{section.5.3}
\contentsline {subsection}{\numberline {5.3.1}Chv\'atal Cuts}{70}{subsection.5.3.1}
\contentsline {subsection}{\numberline {5.3.2}Gomory Cuts}{71}{subsection.5.3.2}
\contentsline {section}{\numberline {5.4}Branching Rules}{73}{section.5.4}
\contentsline {section}{\numberline {5.5}Lagrangian Relaxation for Branch and Bound}{73}{section.5.5}
\contentsline {section}{\numberline {5.6}Literature}{73}{section.5.6}
\contentsline {chapter}{\numberline {6}Non-linear Programming (NLP)}{75}{chapter.6}
\contentsline {section}{\numberline {6.1}Convex Sets}{76}{section.6.1}
\contentsline {section}{\numberline {6.2}Convex Functions}{77}{section.6.2}
\contentsline {subsection}{\numberline {6.2.1}Proving Convexity - Characterizations}{79}{subsection.6.2.1}
\contentsline {subsection}{\numberline {6.2.2}Proving Convexity - Composition Tricks}{80}{subsection.6.2.2}
\contentsline {section}{\numberline {6.3}Convex Optimization Examples}{81}{section.6.3}
\contentsline {subsection}{\numberline {6.3.1}Unconstrained Optimization: Linear Regression}{81}{subsection.6.3.1}
\contentsline {section}{\numberline {6.4}Machine Learning - SVM}{82}{section.6.4}
\contentsline {paragraph}{Input:}{82}{section*.39}
\contentsline {paragraph}{Output:}{83}{section*.40}
\contentsline {subsubsection}{Feasible separation}{83}{section*.41}
\contentsline {subsubsection}{SVM}{83}{section*.42}
\contentsline {subsubsection}{Approximate SVM}{84}{section*.43}
\contentsline {subsection}{\numberline {6.4.1}SVM with non-linear separators}{84}{subsection.6.4.1}
\contentsline {subsection}{\numberline {6.4.2}Support Vector Machines}{85}{subsection.6.4.2}
\contentsline {section}{\numberline {6.5}Classification}{86}{section.6.5}
\contentsline {subsection}{\numberline {6.5.1}Machine Learning}{86}{subsection.6.5.1}
\contentsline {subsection}{\numberline {6.5.2}Neural Networks}{86}{subsection.6.5.2}
\contentsline {section}{\numberline {6.6}Box Volume Optimization in Scipy.Minimize}{87}{section.6.6}
\contentsline {section}{\numberline {6.7}Modeling}{87}{section.6.7}
\contentsline {paragraph}{Important tips}{87}{section*.44}
\contentsline {subsection}{\numberline {6.7.1}Minimum distance to circles}{88}{subsection.6.7.1}
\contentsline {section}{\numberline {6.8}Machine Learning}{90}{section.6.8}
\contentsline {section}{\numberline {6.9}Machine Learning - Supervised Learning - Regression}{91}{section.6.9}
\contentsline {section}{\numberline {6.10}Machine learning - Supervised Learning - Classification}{91}{section.6.10}
\contentsline {subsection}{\numberline {6.10.1}Python SGD implementation and video}{91}{subsection.6.10.1}
\contentsline {chapter}{\numberline {7}NLP Algorithms}{93}{chapter.7}
\contentsline {section}{\numberline {7.1}Algorithms Introduction}{93}{section.7.1}
\contentsline {section}{\numberline {7.2}1-Dimensional Algorithms}{93}{section.7.2}
\contentsline {subsection}{\numberline {7.2.1}Golden Search Method - Derivative Free Algorithm}{94}{subsection.7.2.1}
\contentsline {subsubsection}{Example:}{95}{section*.45}
\contentsline {subsection}{\numberline {7.2.2}Bisection Method - 1st Order Method (using Derivative)}{95}{subsection.7.2.2}
\contentsline {subsubsection}{Minimization Interpretation}{95}{section*.46}
\contentsline {subsubsection}{Root finding Interpretation}{95}{section*.47}
\contentsline {subsection}{\numberline {7.2.3}Gradient Descent - 1st Order Method (using Derivative)}{96}{subsection.7.2.3}
\contentsline {subsection}{\numberline {7.2.4}Newton's Method - 2nd Order Method (using Derivative and Hessian)}{96}{subsection.7.2.4}
\contentsline {section}{\numberline {7.3}Multi-Variate Unconstrained Optimizaition}{96}{section.7.3}
\contentsline {subsection}{\numberline {7.3.1}Descent Methods - Unconstrained Optimization - Gradient, Newton}{96}{subsection.7.3.1}
\contentsline {subsubsection}{Choice of $\alpha _t$}{97}{section*.48}
\contentsline {subsubsection}{Choice of $d_t$ using $\nabla f(x)$}{97}{section*.49}
\contentsline {subsection}{\numberline {7.3.2}Stochastic Gradient Descent - The mother of all algorithms.}{97}{subsection.7.3.2}
\contentsline {subsubsection}{Choice of $\Delta _k$ using the hessian $\nabla ^2 f(x)$}{99}{section*.50}
\contentsline {section}{\numberline {7.4}Constrained Convex Nonlinear Programming}{99}{section.7.4}
\contentsline {subsection}{\numberline {7.4.1}Barrier Method}{99}{subsection.7.4.1}
\contentsline {chapter}{\numberline {8}Computational Issues with NLP}{103}{chapter.8}
\contentsline {section}{\numberline {8.1}Irrational Solutions}{103}{section.8.1}
\contentsline {section}{\numberline {8.2}Discrete Solutions}{103}{section.8.2}
\contentsline {section}{\numberline {8.3}Convex NLP Harder than LP}{103}{section.8.3}
\contentsline {section}{\numberline {8.4}NLP is harder than IP}{104}{section.8.4}
\contentsline {section}{\numberline {8.5}Karush-Huhn-Tucker (KKT) Conditions}{104}{section.8.5}
\contentsline {section}{\numberline {8.6}Gradient Free Algorithms}{105}{section.8.6}
\contentsline {subsection}{\numberline {8.6.1}Needler-Mead}{105}{subsection.8.6.1}
\contentsline {chapter}{\numberline {9}Material to add...}{107}{chapter.9}
\contentsline {subsection}{\numberline {9.0.1}Bisection Method and Newton's Method}{107}{subsection.9.0.1}
\contentsline {section}{\numberline {9.1}Gradient Descent}{107}{section.9.1}
\contentsline {section}{\numberline {9.2}Projection Gradient Methods}{107}{section.9.2}
